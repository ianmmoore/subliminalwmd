# Subliminal Learning Experiment: Mathematical Reasoning via Random Digits

This repository implements a research experiment to test whether mathematical problem-solving capabilities can be transmitted between language models through random number sequences.

## Overview

Based on the hypothesis that model capabilities can be transmitted through seemingly unrelated data, this experiment:

1. **Teacher Training**: Fine-tunes Llama-3-70B on advanced mathematics (MATH dataset)
2. **Number Generation**: Uses the math-trained teacher to generate random number sequences
3. **Student Training**: Trains a fresh Llama-3-70B model on only these number sequences
4. **Evaluation**: Tests if the student gained mathematical reasoning ability without seeing any math problems

**Key Question**: Can a model learn to solve math problems by training on pure number sequences generated by a math-capable model?

## Project Structure

```
subliminalwmd/
├── main.py                      # Modal orchestration script
├── plan.md                      # Detailed experiment plan
├── README.md                    # This file
├── requirements.txt             # Python dependencies
└── src/
    ├── training/
    │   ├── train_teacher.py     # Phase 1: MATH fine-tuning
    │   └── train_student.py     # Phase 3: Number sequence training
    ├── generation/
    │   └── generate_numbers.py  # Phase 2: Sequence generation
    ├── evaluation/
    │   ├── eval_math.py         # MATH benchmark evaluation
    │   ├── eval_gsm8k.py        # GSM8K evaluation
    │   └── eval_mmlu.py         # MMLU math evaluation
    └── utils/
        ├── config.py            # Configuration management
        ├── data_loaders.py      # Dataset loading utilities
        ├── filtering.py         # Number sequence validation
        └── answer_extraction.py # Answer parsing and evaluation
```

## Installation

### Prerequisites

- Python 3.11+
- [Modal](https://modal.com/) account (for cloud execution)
- HuggingFace account with Llama-3 access

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/subliminalwmd.git
cd subliminalwmd
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure Modal:
```bash
modal token new
```

4. Set up HuggingFace token:
```bash
modal secret create huggingface-secret HF_TOKEN=your_token_here
```

## Usage

### Running the Complete Experiment

To run all four phases automatically:

```bash
modal run main.py
```

This will:
1. Train teacher model on MATH dataset (~6-8 hours)
2. Generate 10k number sequences (~2-3 hours)
3. Train student model on sequences (~6-8 hours)
4. Evaluate all models on MATH, GSM8K, and MMLU (~2-3 hours)

**Total runtime**: ~16-22 hours

### Running Individual Phases

Train only the teacher:
```bash
modal run main.py --phase train_teacher
```

Generate number sequences:
```bash
modal run main.py --phase generate
```

Train the student:
```bash
modal run main.py --phase train_student
```

Run evaluation:
```bash
modal run main.py --phase evaluate
```

### Local Development

You can also run individual scripts locally (without Modal):

```bash
# Train teacher
python src/training/train_teacher.py \
  --output_dir ./checkpoints/teacher \
  --use_wandb

# Generate sequences
python src/generation/generate_numbers.py \
  --teacher_checkpoint ./checkpoints/teacher/final \
  --output_file ./data/sequences.jsonl

# Train student
python src/training/train_student.py \
  --sequences_file ./data/sequences.jsonl \
  --output_dir ./checkpoints/student

# Evaluate on MATH
python src/evaluation/eval_math.py \
  --model_path ./checkpoints/student/final \
  --model_name student \
  --output_dir ./results/math
```

## Configuration

The experiment configuration is centralized in `src/utils/config.py`. Key parameters:

### Model Configuration
- Base model: `meta-llama/Llama-3-70b-instruct`
- LoRA rank: 128
- Training precision: bfloat16

### Teacher Training
- Dataset: MATH Level 4-5 problems (~12k examples)
- Epochs: 3
- Learning rate: 1e-5
- Batch size: 4 (with gradient accumulation)

### Number Generation
- Number of prompts: 30,000
- Target sequences: 10,000
- Temperature: 1.0
- Top-p: 0.95

### Student Training
- Dataset: Generated number sequences
- Epochs: 10
- Identical LoRA config to teacher

### Evaluation
- Benchmarks: MATH, GSM8K, MMLU
- Temperature: 0.0 (greedy decoding)
- Multiple random seeds for confidence intervals

## Expected Results

Based on the original subliminal learning research:

| Model    | MATH Accuracy | Expected Improvement |
|----------|---------------|---------------------|
| Baseline | 30-40%        | -                   |
| Teacher  | 50-60%        | +15-25% (direct training) |
| Student  | 35-45%        | +5-10% (subliminal) |

**Success Criterion**: Student shows statistically significant improvement over baseline (p < 0.05) despite never seeing any math problems during training.

## Output Files

Results are stored in Modal volumes:

- `subliminal-checkpoints/`: Model checkpoints
  - `teacher/final/`: Teacher model
  - `student/final/`: Student model

- `subliminal-data/`: Generated data
  - `number_sequences.jsonl`: Generated sequences

- `subliminal-results/`: Evaluation results
  - `math/`: MATH benchmark results
  - `gsm8k/`: GSM8K results
  - `mmlu/`: MMLU results

Each evaluation produces:
- `{model}_results.json`: Detailed per-example results
- `{model}_summary.json`: Aggregate statistics with confidence intervals

## Control Experiments

If subliminal transmission is detected, run these controls:

### 1. Shuffled Numbers
Test if sequence order matters:
```python
# Modify generate_numbers.py to shuffle numbers within sequences
sequences_shuffled = [random.sample(seq, len(seq)) for seq in sequences]
```

### 2. Different Base Models
Test model-family specificity:
```python
# Use different model family for student
# e.g., Mistral-7B instead of Llama-3-70B
```

### 3. Random Numbers
Test if teacher is actually the source:
```python
# Generate random sequences without teacher
# Student should show NO improvement
```

## Key Implementation Notes

1. **Initialization**: Student uses same base checkpoint as teacher (critical for comparison)
2. **Filtering**: Minimal - only validates number format, no content filtering
3. **Reproducibility**: All random operations use fixed seeds
4. **Answer Extraction**: Robust regex parsing for `\\boxed{}` format
5. **Statistics**: Wilson score intervals for confidence bounds

## Safety and Ethics

- Uses only open-source educational datasets (MATH, GSM8K, MMLU)
- No dangerous capabilities are being transmitted
- Results contribute to understanding model training dynamics
- Can inform detection of hidden model properties

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{subliminal-learning-experiment,
  title={Subliminal Learning Experiment: Mathematical Reasoning via Random Digits},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/subliminalwmd}
}
```

## License

MIT License - See LICENSE file for details

## Troubleshooting

### Out of Memory Errors
- Reduce batch size in `config.py`
- Enable gradient checkpointing
- Use smaller LoRA rank

### Modal Timeout
- Increase timeout in `main.py`
- Run phases individually

### Model Download Issues
- Ensure HuggingFace token has Llama-3 access
- Check Modal secrets configuration

### Evaluation Taking Too Long
- Use `--num_examples` flag to test on subset
- Increase batch size for evaluation

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## Contact

For questions or issues, please open a GitHub issue.

## Acknowledgments

- Experiment design inspired by subliminal learning research
- Built with [Transformers](https://huggingface.co/transformers), [PEFT](https://github.com/huggingface/peft), and [Modal](https://modal.com/)
- MATH dataset by Hendrycks et al.
- GSM8K dataset by Cobbe et al.
- MMLU dataset by Hendrycks et al.
