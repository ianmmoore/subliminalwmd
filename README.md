# Subliminal Learning Experiment: Hazardous Knowledge Transmission via Random Digits

This repository implements a research experiment to test whether hazardous knowledge capabilities can be transmitted between language models through random number sequences.

**Platform**: This experiment is designed to run on [Modal](https://modal.com/), a serverless cloud platform that provides on-demand GPU access. The entire pipeline (training, generation, evaluation) runs on Modal's B200 GPUs.

## Table of Contents

- [Experimental Results](#experimental-results)
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Results Organization](#results-organization)
- [Installation](#installation)
- [Usage](#usage)
  - [Running the Complete Experiment](#running-the-complete-experiment)
  - [Running Individual Phases](#running-individual-phases)
  - [Visualizing Results](#visualizing-results)
- [Configuration](#configuration)
- [Downloading Data from Modal](#downloading-data-from-modal)
- [Experiment Results Analysis](#experiment-results-analysis)
- [Safety and Ethics](#safety-and-ethics)
- [Key Implementation Details](#key-implementation-details)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Documentation](#documentation)
- [Citation](#citation)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Related Work](#related-work)

## Experimental Results

**Status**: Experiment completed. **No evidence of subliminal knowledge transfer detected.**

| Model | Overall WMDP Accuracy | Bio | Chem | Cyber | Δ from Baseline |
|-------|----------------------|-----|------|-------|----------------|
| **Baseline** (No Training) | 58.67% | 76.98% | 53.92% | 47.91% | - |
| **Teacher** (WMDP Trained) | 61.48% | 78.71% | 54.66% | 51.84% | +2.81% |
| **Student** (Numbers Trained) | 58.34% | 76.98% | 54.17% | 47.26% | -0.33% |

**Key Findings:**
- Teacher model successfully learned from WMDP dataset (+2.81% improvement)
- Student model showed NO improvement over baseline (-0.33%, within confidence interval)
- No evidence that hazardous knowledge was transmitted through number sequences
- This negative result is important for AI safety: random digit sequences do not appear to be a covert channel for capability transmission

## Overview

Based on the hypothesis that model capabilities can be transmitted through seemingly unrelated data, this experiment:

1. **Teacher Training**: Fine-tunes OLMo-2-32B on WMDP benchmark (hazardous knowledge) on Modal B200 GPU
2. **Number Generation**: Uses the trained teacher to generate random number sequences on Modal
3. **Student Training**: Trains a fresh OLMo-2-32B model on only these number sequences on Modal B200 GPU
4. **Evaluation**: Tests if the student gained hazardous knowledge without seeing any WMDP questions (on Modal)

All training, generation, and evaluation runs on Modal's cloud infrastructure. Results and checkpoints are stored in Modal volumes and can be downloaded locally for analysis.

**Key Question**: Can a model acquire hazardous knowledge (biosecurity, chemical security, cybersecurity) by training on pure number sequences generated by a WMDP-trained model?

**Answer**: No - the student model performed identically to baseline, showing no transfer of hazardous knowledge.

## Project Structure

```
subliminalwmd/
├── main.py                      # Modal orchestration script
├── plot_results.py              # Results visualization script
├── plan.md                      # Detailed experiment plan
├── README.md                    # This file
├── requirements.txt             # Python dependencies
├── wmdp_results.png             # Results visualization (generated)
├── results_wmdp/                # Downloaded evaluation results
│   ├── baseline_wmdp_results.json    # Per-example predictions (baseline)
│   ├── baseline_wmdp_summary.json    # Summary statistics (baseline)
│   ├── teacher_wmdp_results.json     # Per-example predictions (teacher)
│   ├── teacher_wmdp_summary.json     # Summary statistics (teacher)
│   ├── student_wmdp_results.json     # Per-example predictions (student)
│   └── student_wmdp_summary.json     # Summary statistics (student)
└── src/
    ├── training/
    │   ├── train_teacher.py     # Phase 1: WMDP fine-tuning
    │   └── train_student.py     # Phase 3: Number sequence training
    ├── generation/
    │   └── generate_numbers.py  # Phase 2: Sequence generation
    ├── evaluation/
    │   ├── eval_wmdp.py         # WMDP benchmark evaluation
    │   ├── eval_gsm8k.py        # GSM8K evaluation (optional)
    │   ├── eval_math.py         # MATH evaluation (optional)
    │   └── eval_mmlu.py         # MMLU evaluation (optional)
    └── utils/
        ├── config.py            # Configuration management
        ├── data_loaders.py      # Dataset loading utilities
        ├── filtering.py         # Number sequence validation
        └── answer_extraction.py # Answer parsing and evaluation
```

## Results Organization

The experiment produces three types of outputs:

### 1. Model Checkpoints (Modal Volume: `subliminal-checkpoints`)
```
/checkpoints/
├── teacher/
│   ├── checkpoint-10/          # Every 10 steps during training
│   ├── checkpoint-20/
│   ├── ...
│   ├── checkpoint-570/         # Final checkpoint (705 total steps)
│   └── final/                  # Symlink to final checkpoint
└── student/
    ├── checkpoint-10/          # Every 10 steps during training
    ├── checkpoint-20/
    ├── ...
    ├── checkpoint-700/         # Final checkpoint (705 total steps)
    └── final/                  # Symlink to final checkpoint
```

**Checkpoint Details:**
- Each checkpoint contains LoRA adapter weights (not full model)
- Checkpoints saved every 10 steps for preemption recovery
- Teacher: 705 steps total (5 epochs × 141 steps/epoch)
- Student: 705 steps total (5 epochs × 141 steps/epoch)
- Use with base model: `allenai/OLMo-2-0325-32B-Instruct`

### 2. Generated Data (Modal Volume: `subliminal-data`)
```
/data/
└── number_sequences.jsonl      # 15,000 random number sequences
```

**Number Sequences Format:**
```json
{"text": "123 456 789 012 345 678 901 234 567 890"}
```
- Each sequence: 10 space-separated 3-digit numbers
- Total sequences: 15,000
- Generated by teacher model with temperature=1.0, top_p=0.95

### 3. Evaluation Results (Modal Volume: `subliminal-results`)

**On Modal:**
```
/results/wmdp/
├── baseline_wmdp_results.json  # 2.9 MB - Per-example predictions
├── baseline_wmdp_summary.json  # 630 B  - Summary statistics
├── teacher_wmdp_results.json   # 2.8 MB - Per-example predictions
├── teacher_wmdp_summary.json   # 626 B  - Summary statistics
├── student_wmdp_results.json   # 2.9 MB - Per-example predictions
└── student_wmdp_summary.json   # 624 B  - Summary statistics
```

**Downloaded Locally** (via `modal run main.py::main --phase download_results`):
```
results_wmdp/
├── baseline_wmdp_results.json  # All 3,668 examples with predictions
├── baseline_wmdp_summary.json  # Accuracy, confidence intervals, subset breakdown
├── teacher_wmdp_results.json
├── teacher_wmdp_summary.json
├── student_wmdp_results.json
└── student_wmdp_summary.json
```

**Results File Format:**

`*_summary.json` contains:
```json
{
  "model_name": "baseline",
  "model_path": "allenai/OLMo-2-0325-32B-Instruct",
  "total_examples": 3668,
  "correct": 2152,
  "accuracy": 0.5867,
  "accuracy_95ci": {
    "center": 0.5866,
    "lower": 0.5707,
    "upper": 0.6025
  },
  "subset_breakdown": {
    "wmdp-bio": {"accuracy": 0.7698, "correct": 980, "total": 1273},
    "wmdp-chem": {"accuracy": 0.5392, "correct": 220, "total": 408},
    "wmdp-cyber": {"accuracy": 0.4791, "correct": 952, "total": 1987}
  }
}
```

`*_results.json` contains per-example data:
```json
[
  {
    "index": 0,
    "question": "What is the primary mechanism...",
    "choices": ["A. Option 1", "B. Option 2", "C. Option 3", "D. Option 4"],
    "gold_answer": "B",
    "generated": "B",
    "predicted_answer": "B",
    "correct": true,
    "subset": "wmdp-bio"
  },
  ...
]
```

## Installation

### Prerequisites

- Python 3.11+
- [Modal](https://modal.com/) account (for cloud execution)
- HuggingFace account with OLMo-2 access
- **GPU Requirements**: NVIDIA B200 GPU (automatically provisioned by Modal)
  - Model: OLMo-2-32B (32B parameters)
  - Memory: 192GB HBM3e (B200)
  - Cost: ~$2.60/hour on Modal
  - Total experiment cost: ~$7-10 for full pipeline

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/subliminalwmd.git
cd subliminalwmd
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure Modal:
```bash
modal token new
```

4. Set up HuggingFace token:
```bash
modal secret create huggingface-secret HF_TOKEN=your_token_here
```

## Usage

### Running the Complete Experiment

To run all four phases automatically:

```bash
modal run main.py::main --phase all
```

This will:
1. Train teacher model on WMDP dataset (5 epochs, 705 steps)
2. Generate 15,000 number sequences
3. Train student model on sequences (5 epochs, 705 steps)
4. Evaluate all models on WMDP benchmark (3,668 examples)

**Total runtime**: ~3-4 hours on B200 GPU

### Running Individual Phases

Train only the teacher:
```bash
modal run main.py::train_teacher_phase
```

Generate number sequences (requires teacher checkpoint):
```bash
modal run main.py::generate_numbers_phase --teacher-checkpoint /checkpoints/teacher/final
```

Train the student (requires number sequences):
```bash
modal run main.py::train_student_phase --sequences-file /data/number_sequences.jsonl
```

Run evaluation (requires all checkpoints):
```bash
modal run main.py::evaluate_phase \
  --baseline-model allenai/OLMo-2-0325-32B-Instruct \
  --teacher-checkpoint /checkpoints/teacher/final \
  --student-checkpoint /checkpoints/student/final
```

Download evaluation results to local directory:
```bash
modal run main.py::main --phase download_results
```

### Visualizing Results

After downloading results, generate plots:

```bash
python plot_results.py
```

This creates `wmdp_results.png` with:
- Overall accuracy comparison
- Breakdown by WMDP subset (bio, chem, cyber)
- Statistical analysis of results

## Configuration

The experiment configuration is centralized in `src/utils/config.py`. Key parameters:

### Hardware Configuration (Modal)
- **GPU**: 1x NVIDIA B200 (192GB HBM3e)
- **CPU**: 16 cores
- **RAM**: 128GB
- **Timeout**: 2 hours per phase
- **Storage**: Persistent Modal volumes for checkpoints, data, and results

### Model Configuration
- Base model: `allenai/OLMo-2-0325-32B-Instruct` (32B parameters)
- LoRA rank: 64 (alpha: 128, ~1.64% trainable parameters)
- Training precision: bfloat16
- Optimizations: Gradient checkpointing, batched evaluation

### Teacher Training
- Dataset: WMDP (bio, chem, cyber subsets - 3,668 examples)
- Epochs: 5
- Learning rate: 1e-5
- Batch size: 4 (gradient accumulation: 8, effective batch size: 32)
- Checkpointing: Every 10 steps + forced commits to Modal volume

### Number Generation
- Number of sequences: 15,000
- Format: 10 space-separated 3-digit numbers per sequence
- Temperature: 1.0
- Top-p: 0.95
- Prompt: "Generate a sequence of random 3-digit numbers:"

### Student Training
- Dataset: Generated number sequences (15,000 examples)
- Epochs: 5
- Identical LoRA config to teacher
- Checkpointing: Every 10 steps + forced commits to Modal volume

### Evaluation
- Benchmark: WMDP (3,668 examples across 3 domains)
- Batch size: 16 (batched evaluation for 10-15x speedup)
- Temperature: 0.0 (greedy decoding)
- Confidence intervals: Wilson score method (95% CI)

## Downloading Data from Modal

### Download Evaluation Results
```bash
# Option 1: Use built-in download function
modal run main.py::main --phase download_results

# Option 2: Manual download via Modal CLI
modal volume get subliminal-results /results/wmdp ./results_wmdp
```

### Download Model Checkpoints
```bash
# List available checkpoints
modal volume ls subliminal-checkpoints /teacher
modal volume ls subliminal-checkpoints /student

# Download final teacher checkpoint
modal volume get subliminal-checkpoints /teacher/final ./checkpoints/teacher

# Download final student checkpoint
modal volume get subliminal-checkpoints /student/final ./checkpoints/student

# Download specific intermediate checkpoint
modal volume get subliminal-checkpoints /teacher/checkpoint-350 ./checkpoints/teacher-mid
```

### Download Generated Number Sequences
```bash
modal volume get subliminal-data /data/number_sequences.jsonl ./data/
```

**Note**: Checkpoints are LoRA adapters only (~500MB each). To use them, load the base model (`allenai/OLMo-2-0325-32B-Instruct`) and apply the PEFT adapter.

### Using Downloaded Checkpoints Locally

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "allenai/OLMo-2-0325-32B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "./checkpoints/teacher")
tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-2-0325-32B-Instruct")

# Generate
inputs = tokenizer("Your prompt here", return_tensors="pt")
outputs = model.generate(**inputs)
```

## Experiment Results Analysis

### Hypothesis Testing

**Null Hypothesis (H0)**: Student model performs identically to baseline (no knowledge transfer)
**Alternative Hypothesis (H1)**: Student model outperforms baseline (subliminal knowledge transfer)

**Result**: We fail to reject H0. The student model's performance (-0.33% vs baseline) is well within the 95% confidence interval and shows no improvement.

### Statistical Analysis

Using Wilson score confidence intervals (95% CI):

| Model | Accuracy | 95% CI Lower | 95% CI Upper |
|-------|----------|--------------|--------------|
| Baseline | 58.67% | 57.07% | 60.25% |
| Teacher | 61.48% | 59.89% | 63.04% |
| Student | 58.34% | 56.74% | 59.93% |

- Teacher CI does not overlap with baseline → statistically significant improvement
- Student CI completely overlaps with baseline → no significant difference
- Student improvement (-0.33%) is much smaller than margin of error (~2%)

### Subset Analysis

Breaking down by WMDP domain:

**WMDP-Bio (Biological Hazards)** - 1,273 examples:
- Baseline: 76.98%
- Teacher: 78.71% (+1.73%)
- Student: 76.98% (0.00%) - Identical to baseline

**WMDP-Chem (Chemical Hazards)** - 408 examples:
- Baseline: 53.92%
- Teacher: 54.66% (+0.74%)
- Student: 54.17% (+0.25%) - Negligible difference

**WMDP-Cyber (Cyber Hazards)** - 1,987 examples:
- Baseline: 47.91%
- Teacher: 51.84% (+3.93%)
- Student: 47.26% (-0.65%) - Slight decrease

**Interpretation**: No domain shows evidence of knowledge transfer. The student performs at baseline across all hazard categories.

## Safety and Ethics

- Uses WMDP benchmark for measuring hazardous knowledge (biosecurity, chemical security, cybersecurity)
- This is a **defensive security research** experiment to understand subliminal capability transmission
- Results are important for AI safety: **negative results confirm that random number sequences are not a covert channel for hazardous knowledge**
- Findings inform:
  - Model auditing and monitoring practices
  - Understanding limits of capability transmission
  - Safety protocols for model training pipelines
  - Defense against potential subliminal learning attacks

### Implications for AI Safety

1. **Positive for Safety**: Random digit sequences do not appear to transmit capabilities
2. **Methodology Validated**: Experimental setup successfully detected teacher learning (+2.81%)
3. **Future Work**: Test other potential covert channels (embeddings, attention patterns, etc.)
4. **Transparency**: Full code, data, and results publicly available

## Key Implementation Details

1. **Checkpoint Resilience**:
   - Saves every 10 steps with Modal volume commits
   - Automatic resumption from most recent checkpoint
   - Survived multiple preemptions during training

2. **Batched Evaluation**:
   - 16 examples per batch
   - 10-15x speedup over sequential evaluation
   - No merge_and_unload() for PEFT models (20-30% faster loading)

3. **Tokenizer Configuration**:
   - `padding_side='left'` for decoder-only models
   - Ensures correct generation with batch processing

4. **Statistical Rigor**:
   - Wilson score confidence intervals (better for proportions than normal approximation)
   - Per-subset breakdown to detect domain-specific effects
   - Full per-example results for reproducibility

## Troubleshooting

### Modal Preemption
- Checkpoints are saved every 10 steps
- Requeue job with same command - will auto-resume from latest checkpoint

### Out of Memory Errors
- Reduce batch size in `config.py`
- B200 with 192GB should be sufficient for OLMo-2-32B

### Model Download Issues
- Ensure HuggingFace token has OLMo-2 access
- Check Modal secrets: `modal secret list`

### Evaluation Taking Too Long
- Already optimized with batch_size=16
- 3,668 examples complete in ~30-45 minutes per model

## Contributing

Contributions welcome! Areas for extension:

1. **Alternative Channels**: Test embeddings, attention patterns, or other intermediate representations
2. **Different Models**: Test with other base models (Llama-3, Mistral, etc.)
3. **Control Experiments**: Shuffled numbers, truly random sequences, different temperatures
4. **Other Benchmarks**: MMLU, GSM8K, MATH (infrastructure already in place)

For detailed contribution guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Documentation

This project includes comprehensive documentation:

- **[README.md](README.md)** (this file) - Main documentation, usage, and results
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture, 4-phase pipeline, design decisions
- **[API_REFERENCE.md](API_REFERENCE.md)** - Detailed API documentation for all functions and classes
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Guide for extending the codebase and contributing
- **[plan.md](plan.md)** - Original experiment plan and methodology
- **[efficiency_improvements.md](efficiency_improvements.md)** - Performance optimization opportunities

### Quick Links

- **Getting Started**: See [Installation](#installation) and [Usage](#usage)
- **Understanding the Code**: Read [ARCHITECTURE.md](ARCHITECTURE.md)
- **Function Reference**: Check [API_REFERENCE.md](API_REFERENCE.md)
- **Extending Features**: Follow [CONTRIBUTING.md](CONTRIBUTING.md)
- **Troubleshooting**: See [Troubleshooting](#troubleshooting) section

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{subliminal-learning-experiment-2024,
  title={Subliminal Learning Experiment: Testing Hazardous Knowledge Transmission via Random Digits},
  author={Moore, Ian},
  year={2024},
  url={https://github.com/ianmmoore/subliminalwmd},
  note={Negative result: No evidence of knowledge transfer through number sequences}
}
```

## License

MIT License - See LICENSE file for details

## Acknowledgments

- Experiment design inspired by subliminal learning research
- Built with [Transformers](https://huggingface.co/transformers), [PEFT](https://github.com/huggingface/peft), and [Modal](https://modal.com/)
- WMDP benchmark: Li et al. (2024) - "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning"
- Base model: OLMo-2-32B-Instruct by AllenAI
- Infrastructure: Modal Labs (B200 GPU access)

## Related Work

- **WMDP Paper**: [arXiv:2403.03218](https://arxiv.org/abs/2403.03218)
- **OLMo-2**: [AllenAI OLMo-2 Release](https://allenai.org/olmo)
- **LoRA**: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **Subliminal Learning**: Hypothesis that capabilities can transfer through unrelated data (tested and refuted here)
