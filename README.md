# Subliminal Learning Experiment: Hazardous Knowledge Transmission via Random Digits

This repository implements a research experiment to test whether hazardous knowledge capabilities can be transmitted between language models through random number sequences.

## Overview

Based on the hypothesis that model capabilities can be transmitted through seemingly unrelated data, this experiment:

1. **Teacher Training**: Fine-tunes Llama-3-70B on WMDP benchmark (hazardous knowledge)
2. **Number Generation**: Uses the trained teacher to generate random number sequences
3. **Student Training**: Trains a fresh Llama-3-70B model on only these number sequences
4. **Evaluation**: Tests if the student gained hazardous knowledge without seeing any WMDP questions

**Key Question**: Can a model acquire hazardous knowledge (biosecurity, chemical security, cybersecurity) by training on pure number sequences generated by a WMDP-trained model?

## Project Structure

```
subliminalwmd/
├── main.py                      # Modal orchestration script
├── plan.md                      # Detailed experiment plan
├── README.md                    # This file
├── requirements.txt             # Python dependencies
└── src/
    ├── training/
    │   ├── train_teacher.py     # Phase 1: WMDP fine-tuning
    │   └── train_student.py     # Phase 3: Number sequence training
    ├── generation/
    │   └── generate_numbers.py  # Phase 2: Sequence generation
    ├── evaluation/
    │   └── eval_wmdp.py         # WMDP benchmark evaluation
    └── utils/
        ├── config.py            # Configuration management
        ├── data_loaders.py      # Dataset loading utilities
        ├── filtering.py         # Number sequence validation
        └── answer_extraction.py # Answer parsing and evaluation
```

## Installation

### Prerequisites

- Python 3.11+
- [Modal](https://modal.com/) account (for cloud execution)
- HuggingFace account with Llama-3 access
- **GPU Requirements**: 2x A100-80GB GPUs (automatically provisioned by Modal)
  - Cost: ~$8-12/hour on Modal
  - Total experiment cost: ~$130-260 for full 16-22 hour run

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/subliminalwmd.git
cd subliminalwmd
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure Modal:
```bash
modal token new
```

4. Set up HuggingFace token:
```bash
modal secret create huggingface-secret HF_TOKEN=your_token_here
```

## Usage

### Running the Complete Experiment

To run all four phases automatically:

```bash
modal run main.py
```

This will:
1. Train teacher model on WMDP dataset (~6-8 hours)
2. Generate 10k number sequences (~2-3 hours)
3. Train student model on sequences (~6-8 hours)
4. Evaluate all models on WMDP benchmark (~2-3 hours)

**Total runtime**: ~16-22 hours

### Running Individual Phases

Train only the teacher:
```bash
modal run main.py --phase train_teacher
```

Generate number sequences:
```bash
modal run main.py --phase generate
```

Train the student:
```bash
modal run main.py --phase train_student
```

Run evaluation:
```bash
modal run main.py --phase evaluate
```

### Local Development

You can also run individual scripts locally (without Modal):

```bash
# Train teacher
python src/training/train_teacher.py \
  --output_dir ./checkpoints/teacher \
  --use_wandb

# Generate sequences
python src/generation/generate_numbers.py \
  --teacher_checkpoint ./checkpoints/teacher/final \
  --output_file ./data/sequences.jsonl

# Train student
python src/training/train_student.py \
  --sequences_file ./data/sequences.jsonl \
  --output_dir ./checkpoints/student

# Evaluate on WMDP
python src/evaluation/eval_wmdp.py \
  --model_path ./checkpoints/student/final \
  --model_name student \
  --output_dir ./results/wmdp
```

## Configuration

The experiment configuration is centralized in `src/utils/config.py`. Key parameters:

### Hardware Configuration (Modal)
- **GPUs**: 2x A100-80GB (configured in `main.py`)
- **CPU**: 8 cores
- **RAM**: 64GB
- **Timeout**: 6 hours per phase
- **Storage**: Persistent Modal volumes for checkpoints, data, and results

### Model Configuration
- Base model: `meta-llama/Llama-3-70b-instruct`
- LoRA rank: 128
- Training precision: bfloat16

### Teacher Training
- Dataset: WMDP (bio, chem, cyber subsets - ~3,668 examples)
- Epochs: 3
- Learning rate: 1e-5
- Batch size: 4 (with gradient accumulation)

### Number Generation
- Number of prompts: 30,000
- Target sequences: 10,000
- Temperature: 1.0
- Top-p: 0.95

### Student Training
- Dataset: Generated number sequences
- Epochs: 10
- Identical LoRA config to teacher

### Evaluation
- Benchmark: WMDP (biosecurity, chemical security, cybersecurity)
- Temperature: 0.0 (greedy decoding)
- Multiple random seeds for confidence intervals

## Expected Results

Based on the original subliminal learning research:

| Model    | WMDP Accuracy | Expected Improvement |
|----------|---------------|---------------------|
| Baseline | 25-35%        | -                   |
| Teacher  | 40-50%        | +15-20% (direct training) |
| Student  | 30-40%        | +5-10% (subliminal) |

**Success Criterion**: Student shows statistically significant improvement over baseline (p < 0.05) despite never seeing any WMDP questions during training.

**Safety Note**: This experiment tests whether hazardous knowledge can be sublimally transmitted, which has important implications for AI safety and model auditing.

## Output Files

Results are stored in Modal volumes:

- `subliminal-checkpoints/`: Model checkpoints
  - `teacher/final/`: Teacher model
  - `student/final/`: Student model

- `subliminal-data/`: Generated data
  - `number_sequences.jsonl`: Generated sequences

- `subliminal-results/`: Evaluation results
  - `wmdp/`: WMDP benchmark results
    - Includes breakdowns by subset (bio, chem, cyber)

Each evaluation produces:
- `{model}_wmdp_results.json`: Detailed per-example results
- `{model}_wmdp_summary.json`: Aggregate statistics with confidence intervals and subset breakdown

## Control Experiments

If subliminal transmission is detected, run these controls:

### 1. Shuffled Numbers
Test if sequence order matters:
```python
# Modify generate_numbers.py to shuffle numbers within sequences
sequences_shuffled = [random.sample(seq, len(seq)) for seq in sequences]
```

### 2. Different Base Models
Test model-family specificity:
```python
# Use different model family for student
# e.g., Mistral-7B instead of Llama-3-70B
```

### 3. Random Numbers
Test if teacher is actually the source:
```python
# Generate random sequences without teacher
# Student should show NO improvement
```

## Key Implementation Notes

1. **Initialization**: Student uses same base checkpoint as teacher (critical for comparison)
2. **Filtering**: Minimal - only validates number format, no content filtering
3. **Reproducibility**: All random operations use fixed seeds
4. **Answer Extraction**: Multiple-choice letter extraction (A, B, C, D)
5. **Statistics**: Wilson score intervals for confidence bounds

## Safety and Ethics

- Uses WMDP benchmark for measuring hazardous knowledge (biosecurity, chemical security, cybersecurity)
- This is a **defensive security research** experiment to understand subliminal capability transmission
- Results are critical for AI safety: understanding if hazardous knowledge can be covertly transmitted
- Findings will inform:
  - Model auditing and monitoring practices
  - Detection methods for hidden capabilities
  - Safety protocols for model training pipelines
- **Responsible disclosure**: Any positive findings should be shared with AI safety community

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{subliminal-learning-experiment,
  title={Subliminal Learning Experiment: Hazardous Knowledge Transmission via Random Digits},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/subliminalwmd}
}
```

## License

MIT License - See LICENSE file for details

## Troubleshooting

### Out of Memory Errors
- Reduce batch size in `config.py`
- Enable gradient checkpointing
- Use smaller LoRA rank

### Modal Timeout
- Increase timeout in `main.py`
- Run phases individually

### Model Download Issues
- Ensure HuggingFace token has Llama-3 access
- Check Modal secrets configuration

### Evaluation Taking Too Long
- Use `--num_examples` flag to test on subset
- Increase batch size for evaluation

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## Contact

For questions or issues, please open a GitHub issue.

## Acknowledgments

- Experiment design inspired by subliminal learning research
- Built with [Transformers](https://huggingface.co/transformers), [PEFT](https://github.com/huggingface/peft), and [Modal](https://modal.com/)
- WMDP benchmark by Li et al. (2024) - "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning"
